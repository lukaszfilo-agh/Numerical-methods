\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\setlength\parindent{0pt}
\usepackage[margin=0.6in]{geometry}
\usepackage{amssymb}

\title{Metody Numeryczne \\ Baza Pytań}
\author{Łukasz Filo}
\date{Styczeń 2023}

\begin{document}

\maketitle
\renewcommand*\contentsname{Spis Treści}
\tableofcontents

\newpage

\section{Kod binarny, kod U2, działania na liczbach w formacie binarnym}

\subsection{Kod binarny}
\begin{itemize}
    \item zapis za pomocą 0 i 1,
    \item każda liczbę jesteśmy w stanie zapisać kodem binarnym.
    \item Jest podstawą współczesnego systemu reprezentacji liczb.
    \item Zapis liczb za pomocą sumy potęg 2.
    \item Liczby ujemne są problemem, bo trzeba przeznaczyć jeden bit na znak i sprawdzać go przy wykonywaniu działania.
    \item Zapis ten teoretycznie jest bardziej podatny na błędy.
\end{itemize}

\subsection{Działania na liczbach w formacie binarnym}
\begin{itemize}
    \item Dodawanie to tak jak dodawanie w słupku.
    \item Odejmowanie tak samo tylko z pożyczaniem.
    \item Przy mnożeniu liczb binarnych łatwo przekroczyć zakres i pojawia się wtedy przepełnienie.
\end{itemize}

\subsection{Kod U2}
\begin{itemize}
    \item Kod uzupełnienia do 2.
    \item Na początku dajemy liczbę ujemną i później dodajemy do niej dodatnie.
    \item Jest to łatwe do konwersji.
    \item Liczby dodatnie są takie same a ujemne powstają po zanegowaniu wszystkich bitów i do całości dodajemy jeden. Po konwersji odejmowanie staję się de facto dodawaniem
\end{itemize}

\section{Format stałoprzecinkowy i zmiennoprzecinkowy}

\subsection{Format stałoprzecinkowy}
\begin{itemize}
    \item Dodajemy ujemne potęgi dwójki do reprezentacji binarnej nie ma różnic w kodowaniu
    \item dokładność jest określona przez ilość bitów
    \item jest to proste i mało wymagające sprzętowo.
    \item Problemem tej reprezentacji jest dokładność.
    \item Działania wykonuje się tak samo.
    \item Kod U2 dalej działa.
    \item Liczba bitów całkowitej i ułamkowej części musi być równa.
    \item Zapis ten ma problemy z dokładnością, nie da się przedstawić dokładnie np. liczby $0.1$.
\end{itemize}

\subsection{Format zmiennoprzecinkowy}
\begin{itemize}
    \item bardziej zaawansowany
    \item ustandaryzowany IEEE
    \item daje nam w pewnych względach większą dokładność
\end{itemize}

\begin{equation*}
    x = SMB^E
\end{equation*}

\begin{itemize}
    \item[S] - Znak
    \item[M] - Mantysa odpowiadająca za ułamkową część zapisu, określa w jakim zakresie jesteśmy pomiędzy kolejnymi potęgami dwójki, liczba stałoprzecinkowa z przedziału <1,2)
    \item[B] - Podstawa, zazwyczaj 2
    \item[E] - Wykładnik, ujemny lub dodatni, koduje się za pomocą U2 lub przesunięcia
\end{itemize}

\subsection{Działania zmiennoprzecinkowe}
\begin{itemize}
    \item Dodawanie i odejmowanie - sprowadzenie do tej samej potęgi, wyciągamy wspólny wykładnik przed nawias, odejmujemy wykładniki i mnożymy dodajemy lub odejmujemy dziesiętne.
    
    \begin{equation*}
        x_1\pm\ x_2=(M_1\pm\ M_2 B^{E_2-E_1})B^{E_1}
    \end{equation*}
    
    \item Mnożenie i dzielenie - mnożymy znak, części ułamkowe i dodajemy lub odejmujemy wykładniki.
    
    \begin{equation*}
        x_1 x_2=(S_1 S_2)(M_1 M_2)B^{E_1+E_2}
    \end{equation*}
    
    \begin{equation*}
        x_1/x_2=(S_1 S_2)(M_1/M_2) B^{E_1-E_2}
    \end{equation*}
    
    \item Dzielenie jest skomplikowane, wymaga implementacji zajmuje sporo czasu i wymaga przeprowadzenia kilku mnożeń.
\end{itemize}

\subsection{IEEE - Single precision}
\begin{itemize}
    \item W języku C float, a w numpy float32
    \item 8 bitów wykładnika, przesunięty o 127 czyli jest od 1 do 244, zostawione bity na inne rzeczy typu nieskończoność itp.
    \item 24 bity mantysy, koduje się tylko 23 po kropce przed kropką zawsze jest 1
\end{itemize}

\subsection{IEEE - Double precision}
\begin{itemize}
    \item W języku C double, a w numpy float64
    \item 11 bitów wykładnika, przesunięty o 1023 czyli od 1 do 2046, zostawione bity na inne rzeczy typu nieskończoność itp.
    \item 53 bity mantysy, koduje się tylko 52 po kropce przed kropką zawsze jest 1
\end{itemize}

\section{Rodzaje i źródła błędów numerycznych, zaokrąglanie i cyfry znaczące, liczby maszynowe i epsilon maszynowy, błąd reprezentacji}

\subsection{Źródła błędów numerycznych}
\subsubsection{Błędy powstające przy formułowaniu zagadnienia}
\begin{itemize}
    \item Błędy pomiaru
    \item Błędy wynikające z przyjęcia określonych przybliżeń opisu zjawisk fizycznych
\end{itemize}\

\subsubsection{Błędy powstające przy obliczeniach}

Błędy grube (pomyłki):
\begin{itemize}
    \item Błąd przy wpisywaniu wzoru do komputera
    \item Zła implementacja algorytmu
    \item Niewłaściwa kolejność wykonywania działań
\end{itemize}

Błędy metody (obcięcie):
\begin{itemize}
    \item Błędy obcięcia są nieodłącznym elementem obliczeń numerycznych.
    \item Błąd obcięcia jest to błąd wynikający z tego, że do uzyskania dokładnego rozwiązania potrzebujemy wykonać nieskończenie wiele obliczeń.
    \item Wszystkie metody numeryczne mają jakiś błąd metody
    \item Dobre algorytmy podają jednak jego oszacowanie, w ten sposób wiemy jak daleko jesteśmy od rozwiązania nawet jak przerwiemy obliczenia
\end{itemize}

Błędy zaokrągleń: \\
Nieusuwalne w pełni źródło błędów, nad którym mamy mniejszą kontrolę niż nad błędem metody. \\
Liczba $\tilde{y}=\mathrm{rd}(y)$ jest poprawnie zaokrąglona do $d$ miejsc po przecinku, jeżeli:
\begin{equation*}
    \varepsilon=|y-\tilde{y}|\leq\frac{1}{2} \cdot 10^{-d}
\end{equation*}
$k$-tą cyfrę dziesiętną liczby $\tilde{y}$ nazwiemy znaczącą gdy
\begin{equation*}
    |y-\tilde{y}|\leq\frac{1}{2} \cdot 10^{-k}
\end{equation*}
oraz
\begin{equation*}
    |\tilde{y}|\geq10^{-k}
\end{equation*}

\subsection{Liczby maszynowe}
\begin{itemize}
    \item To taka liczba jaką można przedstawić w komputerze. Zbiór tych liczb oznaczamy A
    \item Dokładność maszynową (epsilon maszynowy) - $eps$, definiujemy:
\end{itemize}

\begin{equation*}
    eps=min\{x\in A:fl(1+x)>1,x>0\}
\end{equation*}

\begin{itemize}
    \item[fl] - operacja zmiennoprzecinkowa
\end{itemize}
Innymi słowy, jest to najmniejsza liczba, którą możemy dodać do 1, aby uzyskać coś większego od 1.


\subsection{Epsilon maszynowy}
Zależy on od liczby bitów na część ułamkową:
\begin{itemize}
    \item Single precision: $\varepsilon_m=2^{-24} \approx 5,96 \ldots 10^{-8}$
    \item Double precision: $\varepsilon_m=2^{-52} \approx 1,11 \ldots 10^{-16}$
\end{itemize}

\subsection{Maksymalny błąd reprezentacji}
\begin{itemize}
    \item Dla każdej liczby rzeczywistej $x$ istnieje taka liczba $\varepsilon$, taka że $|\varepsilon|<\varepsilon_m$, że:
    \begin{equation*}
        \mathrm{fl}(x)=x(1+\varepsilon)
    \end{equation*}
    \item Oznacza to, że błąd względny między liczbą rzeczywistą, a jej najbliższą reprezentacją zmiennoprzecinkową jest zawsze mniejszy od $\varepsilon_m$
\end{itemize}

\subsubsection{Przenoszenie się błędów zaokrągleń}
Korzystając z rachunku różniczkowego (różniczkowa analiza błędów) możemy podać wzór na przenoszenie się błędów. Niech $y=\varphi(x_1,x_1,\dots,x_n)$ będzie wielkością, którą chcemy obliczyć a $x_i$ są zaokrąglone z błędem $\varepsilon_{xi}$. Błąd względny wyliczania $y$ wynosi w przybliżeniu:
\begin{equation*}
    \varepsilon_y = \sum_{i=0}^n \frac{x_i}{\varphi(x)} \cdot \frac{\partial\varphi(x)}{\partial x_i} \cdot \varepsilon_{x_i}
\end{equation*}

\subsubsection{Nieunikniony błąd obliczeń}
Ze względu na zaokrąglenia pewnych błędów nigdy nie unikniemy. Nieunikniony błąd wartości składa się z błędu wyliczenia wartości (przeniesienia błędów) oraz samego błędu zaokrąglenia:
\begin{equation*}
    \frac{\Delta y}{y} = \varepsilon_y + eps
\end{equation*}
Błąd jest zależny od obranej przez nas metody obliczeń.

\section{Złożoność obliczeniowa i "O" duże Uwarunkowanie problemu, stała uwarunkowania, stabilność wsteczna}

\subsection{"O" duże}
\begin{itemize}
    \item Mówimy, że dla wielkości zależnej od parametru np.F(n) zachodzi:
    \begin{equation*}
        F(n)=O(G(n))
    \end{equation*}
    \item jeżeli istnieje taka stała C, że przy n zmierzającym do nieskończoności (odpowiednio dużym), mamy:
    \begin{equation*}
        F(n) \leq CG(n)
    \end{equation*}
    \item Jeżeli interesuje nas $O(c)$, gdzie $c$ jest stałą, zależność ta ma zachodzić niezależnie od wielkości parametru.
    \item Mówimy potocznie, gdy błąd jest równy $O(n^2)$, że błąd jest rzędu $n^2$.
\end{itemize}

\subsection{Złożoność obliczeniowa algorytmu}
\begin{itemize}
    \item O złożoności obliczeniowej mówimy jako o liczbie zasobów potrzebnych do wyliczenia algorytmu.
    \item Złożoność określamy zazwyczaj jako funkcję pewnych wielkości wejściowych np. liczby zmiennych lub liczby ograniczeń
    \item Istnieją różne rodzaje złożoności.
\end{itemize}

\subsubsection{Złożoność arytmetyczna}
Liczba operacji arytmetycznych (zazwyczaj mnożeń) potrzebnych do rozwiązania problemu. Zazwyczaj dotyczy to problemów algebry numerycznej, gdzie jesteśmy w stanie oszacować liczbę działań.

\subsubsection{Złożoność czasowa}
Ile potrzeba czasu na wyliczenie algorytmu. Zazwyczaj przy założeniu stałego czasu obliczeń równoważna złożoności arytmetycznej. Przy bardziej skomplikowanych problemach (gdzie pojawia się zarządzanie pamięcią) może się różnić.
Nie używa się tej liczby wprost.

\subsubsection{Złożoność pamięciowa}
Ilość pamięci potrzebnej do realizacji obliczeń.

\subsection{Bardziej złożone problemy}
Przy rozwiązywaniu równań różniczkowych lub optymalizacji za miarę złożoności określa się liczbę wywołań prawej strony równania różniczkowego lub funkcji celu.

\subsection{Uwarunkowanie problemu i stała uwarunkowania}
\begin{itemize}
    \item Mówimy, że problem $f(x)$ jest dobrze uwarunkowany, jeżeli mała zmiana $x$ powoduje małą zmianę w $f(x)$.
    \item Problem jest źle uwarunkowany, jeżeli mała zmiana $x$ powoduje dużą zmianę w $f(x)$.
    \item Miarą uwarunkowania jest stała $\kappa$ (kappa), która (nieformalnie) określa największy iloraz zaburzeń $f(x)$ wywołanych przez najmniejsze zaburzenia $x$.
    \item Stałą $\kappa$ można wyliczyć tylko w niektórych problemach lub można ją oszacować
\end{itemize}

\subsubsection{Dokładność algorytmu}
\begin{itemize}
    \item Algorytm jest dokładny wtedy, kiedy numeryczne rozwiązanie dla dokładnych danych ma względny błąd na poziomie zera maszynowego $\varepsilon_m$.
    \item Zagwarantowanie, że algorytm jest dokładny wg tej definicji jest niezwykle trudne, zwłaszcza dla źle uwarunkowanych problemów. Ponieważ nie da się tego formalnie zweryfikować poza trywialnymi przypadkami.
\end{itemize}

\subsection{Stabilność algorytmu}
Algorytm jest stabilny wtedy, gdy:
\begin{itemize}
    \item Błąd względny reprezentacji zbioru parametrów jest na poziomie zera maszynowego.
    \item Względna różnica pomiędzy rozwiązaniem przybliżonym dla danych dokładnych a rozwiązaniem dokładnym dla danych przybliżonych jest na poziomie zera maszynowego.
\end{itemize}
\textbf{Stabilny algorytm daje prawie dobrą odpowiedź na prawie dobre pytanie.} \\
Jesteśmy w stanie to wykazać, mają pewne zalety, ale nie są idealne.

\subsection{Stabilność wsteczna algorytmu}
\begin{itemize}
    \item Algorytmy, w których jesteśmy w stanie zbadać błąd od tyłu. 
    \item Dla odpowiednio bliskich $x$ zachodzi, że przybliżone rozwiązanie dla dokładnych danych jest równe dokładnemu rozwiązaniu dla danych przybliżonych.
\end{itemize}
\textbf{Stabilny wstecznie algorytm daje prawidłową odpowiedź na prawie dobre pytanie.} \\
Trudniejsze do wykazania, ale ma korzyści. \\
Jeśli algorytm jest stabilny wstecznie, to jego błąd względny pogarsza się proporcjonalnie do stałej uwarunkowania tj. $O(\kappa \varepsilon_m)$. \\
Stabilność wsteczna jest pożądaną cechą, ale nie wszystkie algorytmy ją mają, ale te które ją mają są bardzo szeroko używane.

\section{Interpolacja, interpolacja wielomianowa i jej jednoznaczność}

\subsection{Interpolacja}
Zadanie interpolacji polega na znalezieniu dla zestawu punktów (węzłów):
\begin{equation*}
    \begin{split}
        x_0,x_1,x_2,\ldots ,x_n \\
        y_0,y_1,y_2,\ldots , y_n
    \end{split}
\end{equation*}
Takiej funkcji, że:
\begin{equation*}
    f(x_i)=y_i, \forall 0 \leq i \leq n
\end{equation*}

\subsection{Interpolacja wielomianowa}
\begin{itemize}
    \item Problem znalezienia całej funkcji sprowadza się więc do znalezienia zestawu współczynników wielomianu $p$.
    \item Wielomian ma stopień $n$, gdy mamy $n+1$ węzłów.
\end{itemize}

\subsection{Jednoznaczność interpolacji}
\begin{itemize}
    \item Czy jest inny wielomian q(x) stopnia n, który interpoluje tak samo dobrze jak p(x)?
    \begin{equation*}
        r(x) = p(x) - q(x)
    \end{equation*}
    \item r(x) jest wielomianem
    \item r(x) jest stopnia co najwyżej n
    \item We wszystkich punktach zachodzi:
    \begin{equation*}
        r(x_i)=p(x_i)-q(x_i)=y_i-y_i=0
    \end{equation*}
    \item Skoro wielomian r(x) zeruje się we wszystkich tych punktach musi mieć on postać:
    \begin{equation*}
        r(x) = \underbrace{(x-x_0)(x-x_1)\ldots(x-x_n)}_{\text{n+1 czynników}}
    \end{equation*}
    \item Ponieważ ma być stopnia n lub niższego, jest to niemożliwe więc:
    \begin{equation*}
        r(x)=0\rightarrow\ q(x)\equiv\ p(x)
    \end{equation*}
    \item Jeżeli potrafimy znaleźć wielomian interpolacyjny dla danego zestawu węzłów to niezależnie jak to zrobimy, zrobimy to dobrze.
\end{itemize}

\section{Wzór Lagrange'a i Wzór barycentryczny}

\subsection{Wzór Lagrange'a}
\begin{itemize}
    \item Klasyczny wzór na określenie wielomianu interpolacyjnego.
    \item Najbardziej uniwersalny
    \item Potrzebny do wyprowadzania innych wzorów
    \item Pojęciowo najprostszy
\end{itemize}
Skoro wszystko jedno, to przedstawimy nasz wielomian jako sumę n prostszych wielomianów. Bierzemy takie wielomiany, które dla jednego węzła są równe 1 a dla pozostałych 0. Gdy pomnożymy sobie jego wartość w tym węźle to otrzymamy wartość, gdy będziemy dodawać je do siebie, to każdy z nich pojedynczo będzie składał się w całość i poprzez jednoznaczność będzie dokładnie tym którego szukamy. \\
Minusem wzoru Lagrange’a jest duża złożoność obliczeniowa, wyliczenie wartości wielomianu w punkcie $x$ wynosi $O(n^2)$.

\subsection{Wzór barycentryczny:}
Jest to modyfikacja interpolacji Lagrange’a, do której bierzemy sobie wielomian pomocniczy, który zeruje się we wszystkich węzłach:
\begin{equation*}
    r(x)=(x-x_0)(x-x_1)\ldots(x-x_n)
\end{equation*}

Wagi barycentryczne nie wchodzą w złożoność obliczeniową, bo liczy się je tylko raz, korzystając ze wzoru:
\begin{equation*}
    w_j=\frac{1}{\prod_{k\neq j}(x_j-x_k)},j=0,\ldots,n
\end{equation*}

Funkcje Bazowe:
\begin{equation*}
    l_j(x)=l(x)\frac{w_j}{x-x_j}
\end{equation*}

\subsubsection{Pierwsza forma wzoru barycentycznego}
Wyciągamy $l(x)$ przed nawias i zmniejszamy liczbę mnożeń do $n$. Złożoność $O(n)$.
\begin{equation*}
    L(x)=l(x)\sum_{j=0}^{n}{y_j\frac{w_j}{x-x_j}}
\end{equation*}

\subsubsection{Druga forma wzoru barycentycznego - usprawnienie}
Przeprowadzamy interpolacje funkcji stałej równej 1:
\begin{equation*}
    1=\sum_{j=0}^{n}{l_j(x)}=l(x)\sum_{j=0}^{n}\frac{w_j}{x-x_j}
\end{equation*}

A następnie dzielimy funkcje $L(x)$ przez funkcje stałą równą 1:
\begin{equation*}
    L(x)=\frac{L(x)}{1}=\frac{l(x)\sum_{j=0}^{n}{y_j\frac{w_j}{x-x_j}}}{l(x)\sum_{j=0}^{n}\frac{w_j}{x-x_j}}
\end{equation*}

$l(x)$  się skraca i otrzymujemy drugą formę wzoru barycentrycznego:
\begin{equation*}
    L(x)=\frac{\sum_{j=0}^{n}{y_j\frac{w_j}{x-x_j}}}{\sum_{j=0}^{n}\frac{w_j}{x-x_j}}
\end{equation*}

Wagi muszą być policzone ze wzoru, który jest wyżej. Jego złożoność obliczeniowa jest jeszcze mniejsza niż w pierwszej formie wzoru barycentrycznego. Problemem jest implementacja w podstawowych pakietach python np. w scipy za każdym razem wyliczane są wagi barycentryczne, co wydłuża czas działania, a można je policzyć tylko raz.

\section{Efekt Rungego}
W przypadku węzłów równooodległych może wystąpić tzw. Efekt Rungego. \\
Podczas interpolacji na węzłach równoodległych funkcja na brzegach przedziału może osiągać bardzo duże wartości. Jest tak ponieważ wagi barycentryczne dla węzłów równoodległych są co do wartości bardzo duże na środku przedziału, czyli wielomiany, których zachowanie kontrolujemy na środku przedziału mają bardzo duże wagi w naszej sumie, z tego wynika, że wielomiany odpowiadające za brzegi przedziału będą miały wagi zdecydowanie mniejsze. Zatem w węzłach dostaniemy takie wartości jakie chcemy, ale pomiędzy węzłami wielomian interpolacyjny będzie latać jak chce. Interpolacja wysokiego stopnia na węzłach równoodległych nie ma sensu.

\section{Interpolacja Czebyszewa}

\subsection{Węzły czebyszewa}
Węzły Czebyszewa to części rzeczywiste punktów rozmieszczonych równomiernie na okręgu jednostkowym na płaszczyźnie zespolonej.
\begin{equation*}
    x_j=\Re_j = \frac{1}{2} (z_j+z_j^{-1})= \cos{\frac{j\pi}{n}},0 \leq j \leq n
\end{equation*}

Nie będą to wartości rozłożone równomiernie, będą zagęszczone na krańcach przedziału. \\
Dla wszystkich poza skrajnymi wagi są równe $1$, a dla skrajnych $\frac{1}{2}$. I do tego są pomnożone przez $(-1)^j$. Wagi te są na tym samym poziomie wielkości dla wszystkich węzłów. Przy bardzo dużych $n$ nie ma efektu Rungego, więc można stosować interpolacje wysokiego rzędu.

\subsection{Zbieżność interpolacji Czebyszewa dla różnych funkcji (rząd zbieżności)}

\subsubsection{Dla funkcji różniczkowalnych}
Jeżeli funkcja jest różniczkowalna $v$ razy i jej pochodne do stopnia $(v-1)$ będą absolutnie ciągłe na przedziale $[-1, 1]$ i $v$ pochodna ma skończone wahanie $V$. Wtedy dla wszystkich $n > v$ błąd interpolacji spełnia ($n$ - rząd interpolacji):
\begin{equation*}
    \|f-p_n \| \leq \frac{4V}{\pi v (n-v)^v}
\end{equation*}
\begin{equation*}
    V = \int_{-1}^1{|f'(x)|}dx
\end{equation*}
Im więcej razy jest różniczkowalna tym szybciej nam to zbiega do zera. Lub im większe $n$ tym szybciej zmierza do zera.

\subsubsection{Dla funkcji analitycznych}
Niech funkcja analityczna w przedziale $[-1, 1]$ będzie analitycznie przedłużalna do otwartej elipsy Bernsteina, w której $|f(x)| \leq M$ czyli jest ograniczona przez $M$. To wtedy im większa elipsa analityczności tym szybciej zbiega nam zbieżność interpolacji. Szybkość zbieżności jest wykładnicza.

\section{Szeregi wielomianów i wielomiany ortogonalne}

\subsection{Szeregi wielomianów}

\begin{itemize}
    \item Z twierdzenia Weierstassa wiemy, że wielomiany mogą dowolnie dokładnie przybliżać funkcję ciągłą
    \item W ogólności możemy powiedzieć:
    \begin{equation*}
        f\left(x\right)\approx\sum_{i=0}^{n}{a_iT_i\left(x\right)}
    \end{equation*}
    \item[$T_i(x)$] funkcje bazy
    \item Daną funkcję możemy przybliżyć jako sumę pewnych funkcji bazy z pewnymi współczynnikami.
\end{itemize}

\subsection{Wielomiany ortogonalne}
\begin{itemize}
    \item W oparciu o to jaka jest nasza funkcja wagowa możemy sprawdzić ortogonalność wielomianów.
    \item Znając iloczyn skalarny możemy sprawdzić ortogonalność wielomianów.
    \item W zależności od wybranej funkcji wagowej $f(x)$ dostaniemy różne rodzaje wielomianów:
    \begin{itemize}
        \item $f(x) = 1$ - wtedy są to wielomiany Legendre’a
        \item $f\left(x\right)=\frac{1}{\sqrt{1-x^2}}$ - wtedy są to wielomiany Czebyszewa
    \end{itemize}
\end{itemize}


\section{Wyliczanie szeregu Czebyszewa z wielomianu interpolacyjnego oraz dobór stopnia interpolacji}

\subsection{Wyliczanie szeregu Czebyszewa z wielomianu interpolacyjnego}
Wielomian można przedstawić jako nieskończoną sumę wielomianów Czebyszewa, współczynniki można policzyć z jakiejś dziwnej całki, która jest trudna do liczenia. Da się to na szczęście zrobić inaczej. \\
Aby wyliczyć współczynniki wielomianów możemy korzystać z DFT która ma złożoność $O(n^2)$. Zamienia nam ona ciąg rzeczywisty na zespolony. W wielomianach Czebyszewa możemy wyliczyć współczynniki w oparciu o wartości w punktach, które są wartościami w szeregu Czebyszewa, nadal wymaga to $O(n^2)$ obliczeń. Ale po lekkiej zamianie wzór na współczynniki jest taki jak FFT.

\subsection{Dobór stopnia interpolacji}
Wyliczamy wartości funkcji w $N$ węzłach Czebyszewa, zamieniamy go na szereg Czebyszewa. Sprawdzamy, czy współczynniki maleją do zera maszynowego, jeżeli nie to zwiększamy stopień interpolacji z $N$ na $2N$. A jeżeli tak to znajdujemy największe $n^*$ takie, że współczynniki są większe od zera maszynowego $(\varepsilon_m)$ i wtedy jest to optymalny rząd interpolacji.

\section{Najlepsza aproksymacja wielomianowa}
\begin{itemize}
    \item Szukamy wielomianu, dla którego błąd (nie znany z góry) E będzie ekwioscylował - czyli funkcja $f$ wraz ze wzrostem argumentu oscyluje między wartościami $\pm \|f\|$
    \item Wyznaczamy współczynniki wielomianu i wartości $E$ w przyjętym zestawie punktów. $E$ niekoniecznie jest maksimum błędu. Wielomian spełnia równanie:
    \begin{equation*}
        b_0+b_1x_i+b_2x_i^2+\ldots+b_nx_i^n+\left(-1\right)^iE=f\left(x_i\right)
    \end{equation*}
    \item Znajdujemy miejsca zerowe krzywej błędu. Wyznaczamy maksima i minima błędu pomiędzy miejscami zerowymi jako nowe punkty $(x_i)$.
    \item Krzywa błędu to różnica między funkcją a jej aproksymacją wielomianową.
    \item Przestajemy, gdy wartości minimów i maksimów są sobie bliskie co do modułu.
\end{itemize}

Do wyznaczania aproksymacji używa się Algorytmu Remeza:
\begin{itemize}
    \item Na początek trzeba wyznaczyć wartość funkcji w $n + 2$ węzłach Czebyszewa $(x_i)$
    \item Wyznaczamy wielomian $p$ i błąd $E$ rozwiązując układ równań - czyli szukamy wielomianu, który w węzłach Czebyszewa będzie odległy o $E$:
\end{itemize}
\begin{equation*}
        \begin{bmatrix}
            1 & x_0 & x_0^2 & \cdots & x_0^n & 1 \\
            1 & x_1 & x_1^2 & \cdots & x_1^n & -1\\
            1 & x_2 & x_2^2 & \cdots & x_2^n & 1 \\
            \vdots & \vdots & \vdots & \ddots & \ddots & \vdots \\
            1 & x_{n+1} & x_{n+1}^2 & \cdots & x_{n+1}^n & (-1)^{n+1}
        \end{bmatrix}
        \begin{bmatrix}
            b_0 \\
            b_1 \\
            \vdots \\
            b_n \\
            E
        \end{bmatrix}
        =
        \begin{bmatrix}
            f(x_0) \\
            f(x_1) \\
            \vdots \\
            f(x_{n+1}) \\
        \end{bmatrix}
    \end{equation*}

\begin{itemize}
    \item Wyznaczamy miejsca zerowe $f - p$. Wyznaczamy maksima i minima $E_j$ pomiędzy miejscami zerowymi i brzegami przedziału, tworzą one nowy zestaw punktów $(x_i)$.
    \item Szukamy największego maksimum $E_M$ i najmniejszego minimum $E_m$. Jeżeli $E_M - E_m < \varepsilon$ to koniec, a jeżeli nie to wracamy do ponownego rozwiązania układu równań.
\end{itemize}

\section{Stała Lebesgue'a i jakość aproksymacji}
\begin{itemize}
    \item Stała Lebesgue’a określa nam jak duży może być błąd maksymalny między punktami interpolacji.
    \item Mówi nam ona o tym jak bardzo aproksymacja funkcji zwiększy normę tej funkcji.
    \item Mówi jak duży błąd może przejść przez naszą aproksymacje czyli ile psujemy przez aproksymacje.
    \item Obliczenie najlepszej aproksymacji może być kłopotliwe.
    \item Im mniejsza stała Lebesgue'a tym lepiej.
    \item Stała Lebesgue’a dla każdej aproksymacji dąży do nieskończoności wraz ze wzrostem $n$.
\end{itemize}

\section{Geometryczna interpretacja układu równań}
\begin{itemize}
    \item Polega na narysowaniu prostych tworzących układ równań i znalezieniu ich przecięcia.
    \item Polega na znalezieniu wektora $x$ który po przemnożeniu przez kolumny macierzy $A$ skonstruuje nam wektor rozwiązań $b$.
    \item Chcemy z wyniku kombinacji liniowej uzyskać jej współczynniki.
    \item Mnożenie $Ax$ to kombinacja liniowa kolumn macierzy $A$ o współczynnikach należących do  wektora $x$. My rozwiązując układ równań poszukujemy tych współczynników, które pozwolą nam zrekonstruować wektor $b$ za pomocą kolumn macierzy $A$ - inaczej jak dany wektor $b$ da się zbudować z macierzy $A$?
\end{itemize}


\section{Rozkład na wartości singularne. Związek ze stałą uwarunkowania. Przykładowe zastosowania.}

\subsection{Rozkład na wartości singularne - SVD}
Działając macierzą A na zbiór wektorów ortogonalnych Q dostaniemy zbiór innych wektorów ortogonalnych P o pewnych długościach:
\begin{equation*}
    \begin{bmatrix}
        &&&& \\
        &&&& \\
        &&A&& \\
        &&&& \\
        &&&& \\
    \end{bmatrix}
    \begin{bmatrix}
        q_1 & q_2 & \cdots & q_n
    \end{bmatrix}
    =
    \begin{bmatrix}
        &&& \\
        &&& \\
        p_1 & p_2 & \cdots & p_n  \\
        &&& \\
        &&& \\
    \end{bmatrix}
    \begin{bmatrix}
        \sigma_1 &&& \\
        & \sigma_2 && \\
        && \dots &  \\
        &&& \sigma_n \\
    \end{bmatrix}
\end{equation*}
\begin{equation*}
    AQ = P\Sigma
\end{equation*}
$\sigma_1, \cdots, \sigma_n$ wartości singularne - to nieujemne uporządkowane liczby rzeczywiste, są one osiami elipsy. \\

Macierze $P$ i $Q$ są unitarne (ortogonalne, $Q^{-1} = Q^T = Q^*$) z czego wynika, że macierz $A$ o wymiarach $m \times n$ po pomnożeniu przez macierz $Q^*$ można rozłożyć następująco:
\begin{equation*}
    A = P \Sigma Q^*
\end{equation*}
\begin{itemize}
    \item[$P$] - macierz $m \times m$ 
    \item[$Q^*$] - macierz $n \times n$
    \item[$\Sigma$] - macierz diagonalna $m \times n$, czyli ma wartości singularne na przekątnej, od największej do najmniejszej
\end{itemize}

Ważne cechy:
\begin{itemize}
    \item Rozkład na wartości singularne zachodzi dla każdej macierzy
    \item Rząd macierzy to liczba niezerowych wartości singularnych
    \item Wyznacznik macierzy kwadratowej jest iloczynem wartości singularnych
    \item Wartości singularne macierzy odwrotnej są odwrotnościami wartości singularnych oryginalnej macierzy.
\end{itemize}

\subsection{Związek ze stałą uwarunkowania}
Dla układu n równań z n niewiadomymi można obliczyć stałą uwarunkowania $\kappa$, będącą ilorazem największej do najmniejszej wartości singularnej macierzy.
\begin{equation*}
    \kappa = \frac{\sigma_1}{\sigma_n}
\end{equation*}

\subsection{Przykładowe zastosowania}
Analiza danych:
\begin{itemize}
    \item Metoda komponentów podstawowych, wyłuskiwanie najbardziej istotnych elementów ze zbioru danych.
\end{itemize}

Kompresja obrazu:
\begin{itemize}
    \item Każdy obraz można przedstawić jako macierz intensywności kolorów (w odcieniach szarości łatwiej) 
    \item Dla macierzy można zastosować SVD 
    \item Jeżeli wartości singularne są malejące, to od pewnego momentu można je zastąpić zerami. 
    \item Wtedy można zastąpić też odpowiadające im wektory z P i Q zerami, bo i tak zostałyby przemnożone przez zero, czyli zajmujemy mniej pamięci. 
    \item Wymiary macierzy się nie zmienią.
\end{itemize}
Nie stosuję się tego, ponieważ jest to niewydajna kompresja.

\section{Rozkład LU (z przestawieniami i bez), uwarunkowanie}

\begin{itemize}
    \item Najpopularniejszy sposób rozwiązywania układów równań liniowych: $A = LU$
    \item Jest to rozkład macierzy $A$ na iloczyn macierzy $L$ - trójkątnej dolnej z jedynkami na przekątnej i macierzy trójkątnej górnej $U$.
    \item Istnieje dla każdej kwadratowej macierzy nieosobliwej.
    \item Przy jego użyciu można w łatwy sposób obliczyć wyznacznik macierzy $A$.
    \item Konstruuje się go przy użyciu eliminacji Gaussa.
    \item Czyścimy kolumny macierzy $A$, aby została nam macierz trójkątna.
    \item Jest to dość dokładny algorytm.
    \item Gdy w macierzy będą zera w niewłaściwych miejscach to eliminacja będzie niemożliwa
    \item Gdy w macierzy zamiast zer będą bardzo małe liczby, to jest wtedy duży potencjał na błędy numeryczne
\end{itemize}

\begin{equation*}
    \underbrace{L_{n-1}\dots L_2 L_1}_{L^{-1}}A=U
\end{equation*}
\begin{equation*}
    L=L_1^{-1}L_2^{-1} \dots L_{n-1}^{-1}
\end{equation*}
\begin{equation*}
    \underset{A}{
    \begin{bmatrix}
        \times & \times & \times & \times \\
        \times & \times & \times & \times \\
        \times & \times & \times & \times \\
        \times & \times & \times & \times \\
    \end{bmatrix}}
    \xrightarrow{L_1}
    \underset{L_1A}{
    \begin{bmatrix}
        \times & \times & \times & \times \\
        \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
        \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
        \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
    \end{bmatrix}}  
    \xrightarrow{L_2}
    \underset{L_2L_1A}{
    \begin{bmatrix}
        \times & \times & \times & \times \\
         & \times & \times & \times \\
         & \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
         & \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
    \end{bmatrix}}
    \xrightarrow{L_3}
    \underset{L_3L_2L_1A}{
    \begin{bmatrix}
        \times & \times & \times & \times \\
         & \times & \times & \times \\
         & & \times & \times \\
         & & \textcolor{red}{0} & \textcolor{red}{\times} \\
    \end{bmatrix}}
\end{equation*}

\begin{itemize}
    \item Przemnażamy macierz $A$ przez taką macierz $L_1$, aby nie ruszyć pierwszego wiersza, wyzerować pierwszą kolumnę, a reszta nie ma znaczenia itd. aż do ostatniego wiersza.
    \item Macierz U to macierz A pomnożona kolejno przez macierze $L_1, \dots, L_{n-1}$.
    \item Macierz L to iloczyn poszczególnych odwrotności $L_1, \dots, L_{n-1}$.
    \item Macierze $L_1, \dots, L_{n-1}$ mają nad przekątną zera są trójkątne a na przekątnej same $1$, i poza wyrazami, na których pracujemy też są same zera. Jest to konstrukcja jak eliminacja Gaussa, tylko skonstruowana jako ciąg przekształceń macierzowych.
    \item Macierz $L$ wygląda następująco:
\end{itemize}
\begin{equation*}
    L = L_1^{-1}L_2^{-1} \cdots L_{n-1}^{-1} = 
    \begin{bmatrix}
        1 & & & & \\
        l_{21} & 1 & & & \\
        l_{21} &  l_{32} & 1 & & \\
        \vdots & \vdots & \ddots & \ddots & \\
        l_{n1} &  l_{n2} & \hdots & l_{n,n-1} & 1\\
    \end{bmatrix}
\end{equation*}

\begin{itemize}
    \item Dzięki prostej konstrukcji macierzy $L_1, \dots , L_{n-1}$ konstrukcja macierzy $L$ jest prosta, bo nie trzeba nic odwracać tylko w odpowiednie miejsca wpisać odpowiednie wyrazy.
\end{itemize}

\subsection{Pivoting}
Wybieramy wiersz zaczynający się od największego elementu i zamieniamy go z bieżącym.

\begin{equation*}
    \begin{bmatrix}
        \times & \times & \times & \times & \times \\
        & \times & \times & \times & \times \\
        & \times & \times & \times & \times \\
        & \textcolor{red}{\alpha_{ik}} & \textcolor{red}{\times} & \textcolor{red}{\times}& \textcolor{red}{\times} \\
        & \times & \times & \times & \times \\
    \end{bmatrix}
    \xrightarrow{P_1}
    \begin{bmatrix}
        \times & \times & \times & \times & \times \\
        & \textcolor{red}{\alpha_{ik}} & \textcolor{red}{\times} & \textcolor{red}{\times}& \textcolor{red}{\times} \\
        & \times & \times & \times & \times \\
        & \times & \times & \times & \times \\
        & \times & \times & \times & \times \\
    \end{bmatrix}
    \xrightarrow{L_1}
    \begin{bmatrix}
        \times & \times & \times & \times & \times \\
        & \alpha_{ik} & \times & \times & \times \\
        & \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
        & \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
        & \textcolor{red}{0} & \textcolor{red}{\times} & \textcolor{red}{\times} & \textcolor{red}{\times} \\
    \end{bmatrix}
\end{equation*}
\begin{equation*}
    L_{n-1}P_{n-1} \cdots L_2P_2L_1P_1A = U
\end{equation*}

Dokonując przestawień na przemian z mnożeniem przez macierz trójkątną dostaniemy macierz trójkątną górną $U$, ponieważ macierz $P$ jest ortogonalna, to nasz iloczyn ma postać: $PA = LU$.

\begin{itemize}
    \item Złożoność obliczeniowa rozkładu LU to $O(\frac{2}{3} n^3)$
    \item LU z pivotingiem jest stabilna wstecznie (stała uwarunkowania mnoży rząd błędu) dla wszystkich praktycznych macierzy
    \item Macierz $P$ posiada w każdej kolumnie jedną $1$.
    \item Stała uwarunkowania układu równań wynosi: $\kappa=\frac{\sigma_1}{\sigma_n}$ 
    \item Istnieją macierze, w których elementy $A$ i $U$ mają różne rzędy wielkości ($U$ dużo większy), takie macierze muszą mieć bardzo specyficzną strukturę i praktycznie nie występują w zastosowaniach
\end{itemize}

\section{Rozkład Choleskiego}

\begin{itemize}
    \item Jest to usprawniony rozkład LU, ale zachodzi tylko dla dodatnio określonych symetrycznych macierzy, czyli takich gdzie: $A = A^T$.
    \item Macierz dodatnio określona jest zawsze symetryczna i forma kwadratowa określona w tej macierzy jest dodatnio określona, czyli dla dowolnego niezerowego wektora $x$ zachodzi: $xAx^T>0$.
    \item Wartości własne macierzy symetrycznej są rzeczywiste.
    \item Wartości własne macierzy dodatnio określonej są zawsze dodatnie.
    \item Wartości singularne macierzy symetrycznej są jej wartościami własnymi.
    \item Rozkład polega na wielokrotnej symetrycznej eliminacji Gaussa z obu stron jednocześnie.
    \item Otrzymujemy po drodze macierze $R_i$ i $R_i^*$
    \item Wykonujemy eliminacje aż do momentu kiedy w miejscu macierzy $A$ uzyskamy macierz jednostkową, więc będziemy mogli pominąć ją w obliczeniach.
    \item Uzyskamy w taki sposób rozkład Choleskiego, macierz $R$ trójkątną górną i jej sprzężenie $R^*$ trójkątną dolną.
\end{itemize}

\begin{equation*}
    A = 
    \begin{bmatrix}
        a_{11} & w^* \\
        w & K \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        \alpha & 0 \\
        \frac{w}{\alpha} & I \\
    \end{bmatrix}
    \begin{bmatrix}
        1 & 0 \\
        0 & K - \frac{K-ww^*}{a_{11}}
    \end{bmatrix}
    \begin{bmatrix}
        \alpha & \frac{w^*}{\alpha} \\
        0 & I \\
    \end{bmatrix}
    = R_1^*A_1R_1
\end{equation*}

\begin{equation*}
    A= \underbrace{R_1^* R_2^* \dots R_m^*}_{R^*} \underbrace{R_1 R_2 \dots R_m}_{R}
\end{equation*}

\begin{equation*}
    A = R^*R, r_{jj} > 0
\end{equation*}

\begin{itemize}
    \item Działa to wtedy, kiedy lewy górny element macierzy jest dodatni, a jest on dodatni, ponieważ macierz $A$ jest dodatnio określona.
    \item Złożoność obliczeniowa to $O(\frac{1}{3} m^3)$ czyli dwa razy szybciej niż eliminacja Gaussa (rozkład LU).
    \item Rozkład Choleskiego jest stabilny wstecznie. Wszystkie problemy występujące przy rozkładzie LU go nie dotyczą, ze względu na dodatnio-określoność.
\end{itemize}

\section{Rozkład QR}
\begin{itemize}
    \item Ideą rozkładu QR jest znalezienie takiej macierzy unitarnej $Q$, której kolumny $q_1, q_2, q_3, \dots$ będą generować te same podprzestrzenie co kolumny macierzy $A$.
    \item Wektory $q_i$ są do siebie ortogonalne i mają długość $1$ (macierz $Q$ jest unitarna) a dodatkowo generują te same podprzestrzenie co wektory $a_i$
    \item Macierz $R$ ma postać macierzy trójkątnej górnej. Jej elementy w pewien sposób skalują kolumny macierzy $Q$, aby uzyskać z nich kolumny macierzy $A$.
\end{itemize}

\begin{equation*}
    \begin{bmatrix}
        & & & \\
        & & & \\
        a_1 & a_2 & \dots & a_n \\
        & & & \\
        & & & \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        & & & \\
        & & & \\
        q_1 & q_2 & \dots & q_n \\
        & & & \\
        & & & \\
    \end{bmatrix}
    \begin{bmatrix}
        r_{11} & r_{12} & \cdots & r_{1n} \\
        & r_{22} & & \vdots \\
        & & \ddots & \vdots \\
        & & & r_{nn} \\
    \end{bmatrix}
\end{equation*}

\begin{itemize}
    \item Każda macierz ma rozkład QR
    \item Każda macierz pełnego rzędu ma jednoznacznie określony rozkład QR, gdzie $r_{ii} > 0$.
\end{itemize}

\section{Ortogonalizacja Grama-Schmidta (ogólna zasada)}

\begin{itemize}
    \item Polega na znalezieniu wektora $q_j$ który zawiera się w podprzestrzeni $a_1, \dots , a_j$ i jest ortogonalny do wszystkich wektorów $q_1, \dots , q_{j-1}$
    \item Wektor ten można skonstruować usuwając wszystko co leży na poprzednich wektorach poprzez odjęcie od wektora $a_j$ wszystkich jego rzutów na poprzednie wektory $q$, czyli iloczynów skalarnych tych wektorów, otrzymamy w taki sposób wektor ortogonalny do wektorów $q_1, \dots , q_{j-1}$.
    \item Otrzymany wektor nie jest ortonormalny, ale można go znormalizować poprzez podzielenie go przez jego normę (długość).
    \item Współczynniki $r_{ij}$ wylicza się za pomocą iloczynów skalarnych.
    \item Współczynniki na przekątnej to odpowiednie długości (normy).
\end{itemize}

\subsection{Usprawnienie ortogonalizacji Grama-Schmidta}
\begin{itemize}
    \item Zamiast robić rzuty na poszczególne wektory, lepiej zrobić rzut na cała przestrzeń, poza wybranym wektorem.
    \item Analitycznie to samo, ale przez inną kolejność działań jest numerycznie bardziej stabilne.
    \item W celu przeprowadzenia faktoryzacji QR w oparciu o faktoryzację Grama-Schmidta konieczne jest wykonanie $O(2mn^2)$ obliczeń.
    \item Rozkład QR wykonywany zmodyfikowanym algorytmem Grama-Schmidta jest stabilny wstecznie.
\end{itemize}

Ortogonalizację Grama-Shmidta można traktować, jako sekwencję operacji mnożenia przez macierze trójkątne tak, aby uzyskać macierz z ortogonalnymi kolumnami:
\begin{equation*}
    A \underbrace{R_1R_2 \dots R_n}_{R^{-1}} = Q
\end{equation*}
Jest to trójkątna ortogonalizacja.

\section{Triangularyzacja Hausholdera (ogólna zasada)}

\begin{itemize}
    \item Przeprowadzenie sekwencji mnożeń przez macierze unitarne w celu uzyskania macierzy trójkątnej.
    \item Jest ortogonalną triangulyzacją.
    \begin{equation*}
        \underbrace{Q_n \dots Q_2Q_1}_{Q^*} A = R
    \end{equation*}
    \item Mniej więcej to samo co eliminacja Gaussa. Szukamy wektorów które wyczyszczą nam macierz i zapewnią nam ortogonalność wektorów w macierzy $Q$.
\end{itemize}
\begin{equation*}
    \begin{bmatrix}
        \times & \times & \times \\
        \times & \times & \times \\
        \times & \times & \times \\
        \times & \times & \times \\
        \times & \times & \times \\
    \end{bmatrix}
    \xrightarrow{Q_1}
    \begin{bmatrix}
        \times & \times & \times \\
        0 & \times & \times \\
        0 & \times & \times \\
        0 & \times & \times \\
        0 & \times & \times \\
    \end{bmatrix}
    \xrightarrow{Q_2}
    \begin{bmatrix}
        \times & \times & \times \\
        & \times & \times \\
        & 0 & \times \\
        & 0 & \times \\
        & 0 & \times \\
    \end{bmatrix}
    \xrightarrow{Q_3}
    \begin{bmatrix}
        \times & \times & \times \\
        & \times & \times \\
        & & \times \\
        & & 0 \\
        & & 0 \\
    \end{bmatrix}
\end{equation*}

\begin{itemize}
    \item Nie wymuszamy, żeby te macierze były trójkątne, możemy je tak ukształtować, żeby były one ortogonalne.
    \item Możemy go robić do macierzy prostokątnych.
    \item Robi się to za pomocą odbić Hausholdera
    \item W celu przeprowadzenia faktoryzacji QR w oparciu o triangularyzacje Hausholdera musimy wykonać \\ $O(2mn^2 - \frac{2}{3}n^3)$ obliczeń. Jest lepsze dla macierzy prostokątnych.
    \item Rozkład QR wykonywany Triangularyzacją Hausholdera jest stabilny wstecznie.
\end{itemize}

\section{Problem najmniejszych kwadratów (metody rozwiązywania)}
\begin{itemize}
    \item Minimalizacja normy residuum układu równań z macierzą prostokątną.
    \item Układ równań z macierzą prostokątną jest zazwyczaj sprzeczny.
    \item Szukamy rozwiązania, które będzie najmniej odbiegać od sprzecznego układu równań.
    \item Rozwiązanie dokładne jest albo bezużyteczne albo nie istnieje.
\end{itemize}

\subsection{Interpretacja geometryczna}
\begin{itemize}
    \item Chcemy znaleźć rozwiązanie układu równań $Ax=b$, gdzie $A \in \mathbb{R}^{m \times n},b\in \mathbb{R}^{m},x\in \mathbb{R}^{n}$
    \item Geometrycznie chcemy skonstruować wektor $b$ z kolumn macierzy $A$.
    \item Jeżeli $b$ nie jest liniowo zależne od kolumn $A$ nie jest to możliwe.
    \item Oznacza to, że wektor $b$ składa się więc z części, którą można przedstawić za pomocą kolumn macierzy $A$ oraz z części, dla której to nie jest możliwe.
    \item Część, której nie da się przedstawić to residuum $r = b - Ax$
    \item Residuum jest ortogonalne, bo nie dało go się skonstruować z kolumn macierzy $A$.
    \item Po przekształceniu przechodzimy z problemu najmniejszych kwadratów do rozwiązania układu równań liniowych.
\end{itemize}

\subsection{Interpretacja optymalizacyjna}
\begin{itemize}
    \item Szukamy minimum kwadratu normy
    \item Po przekształceniach dostajemy taki sam wzór jak w interpretacji geometrycznej: $A^* Ax=A^* b$
\end{itemize}

\subsection{Praktycznie rozwiązanie}
\begin{itemize}
    \item Można rozwiązać układ równań normalnych za pomocą rozkładu Choleskiego, \emph{ale nie robi się tego, bo to zły pomysł $\sim$ JB}
    \item Możemy także to zrobić za pomocą SVD - rozkładu na wartości singularne, bo każda macierz posiada taki rozkład.
\end{itemize}

\begin{equation*}
    \begin{split}
        A & = U \Sigma V^* \\
        V \Sigma U^* U \Sigma V^* & = V \Sigma U^*b \\
        \Sigma V^*x & = U^*b \\
        \Sigma y & = U^*b \\
        x & = Vy
    \end{split}
\end{equation*}

\begin{itemize}
    \item Można też rozwiązać to za przy użyciu rozkładu QR.
\end{itemize}

\begin{equation*}
    \begin{split}
        A & = QR \\
        R^*Q^*QRx & = R^*Q^*b \\
        Rx & = Q^* b
    \end{split}
\end{equation*}

\section{Macierze rzadkie}
\begin{itemize}
    \item W rzeczywistych problemach ilość danych jest na tyle duża, że nie zapisuje się macierzy w pamięci wprost.
    \item Są to macierze diagonalnie zdominowane z czego wynika, że ich wartości własne są niezerowe.
\end{itemize}
Do zapisu macierzy rzadkich używamy:
\begin{itemize}
    \item Dictionary of Keys - odwzorowanie (wiersz, kolumna) $\rightarrow$ wartość (dobry do tworzenia macierzy).
    \item Lista list (LIL) (np. kolumnami).
    \item Współrzędne i wartość (COO), sortowane po wierszach a następnie kolumnach.
    \item Format Yale (Compressed sparse row) (trzy macierze jednowymiarowe).
    \item Wykorzystuje się symetrię macierzy (trzeba zapisać tylko połowę danych)
    \item Wykorzystuje się struktury, takie jak: macierze Hessenberga, macierze Pasmowe.
\end{itemize}


\section{Algorytm GMRES, przestrzenie Kryłowa}
\subsection{Algorytm GMRES}
\begin{itemize}
    \item Generalized Minimal RESiduals 
    \item W $n$-tym kroku szukamy aproksymacji rozwiązania równania za pomocą wektora $x_n \in K_n$, czyli elementów pochodzących z przestrzeni Kryłowa, $n$-te przybliżenie składa się z $n$ kolumn macierzy Kryłowa. Aby wyliczyć rozwiązanie minimalizujemy normę residuum: $r_n=b-Ax_n$.
    \item Po minimalizacji normy nasze przybliżenie rozwiązania będzie ortogonalne do pozostałych kolumn macierzy Kryłowa.
    \item Jest to przeformułowanie problemu do problemu najmniejszych kwadratów.
    \item Polega on na znalezieniu takiego rozwiązania, które w wektorach $b, Ab, \dots$ upcha wszystko tak, że reszta pozostanie ortogonalna.
    \item Chcemy znaleźć najlepszy wektor z przestrzeni Kryłowa, który zminimalizuje macierz $A$.
    \item Idea która chcemy zrobić to Iteracja Arnoldiego, która rozwiązuje nam problem najmniejszych kwadratów poprzez sukcesywny rozkład QR macierzy $K$.
    \item Na koniec algorytm sprowadza się do rozwiązania układu prostokątnego $(n+1) \times n$, który jest opisany macierzą Hessenberga od której odejmujemy prostszy wektor $b$.
    \item Ostatni problem ma wymiar $(n+1) \times n$
    \item Wychodzimy z problemu $m \times m$ i zwiększając $n$ wyliczamy kolejne normy residuum aż będzie ono zadowalająco małe.
\end{itemize}

\subsection{Metody podprzestrzeni Kryłowa}
\begin{itemize}
    \item Rozwiązań szukamy w wektorach należących do podprzestrzeni Kryłowa.
    \item Dla macierzy rzadkich nie jest kosztowne obliczeniowo, bo wymaga mnożeń tylko tam, gdzie są niezerowe wyrazy.
    \item Rozwiązanie jest konstruowane jako pewna kombinacja wektorów postaci:
    \begin{equation*}
        b,Ab,A^2b,A^3b, \dots
    \end{equation*}
    \item Podprzestrzeń skonstruowana z takich wektorów nazywana jest przestrzenią Kryłowa, przy czym:
    \begin{equation*}
        K_n= \langle b,Ab,A^2 b,A^3 b, \dots ,A^{n-1} b \rangle
    \end{equation*}
\end{itemize}

\section{Metody bisekcji, siecznych, regula falsi, metoda Newtona}

\subsection{Metoda bisekcji}
\begin{itemize}
    \item Szukamy miejsca zerowego w jakimś przedziale.
    \item Funkcja musi przyjmować przeciwne znaki na krańcach przedziału.
    \item Tniemy na pół i sprawdzamy która połówka ma przeciwne znaki na krańcach przedziału.
    \item Robimy to do momentu aż osiągniemy oczekiwaną przez nas dokładność, która określa nakład pracy jaki należy wykonać.
    \item Stałe zawężanie przedziału, każdy następny krok jest mniejszy od poprzedniego.
    \item Nie wykorzystujemy żadnych informacji o funkcji, przydaje się jeśli nie mamy tych informacji.
\end{itemize}

\subsection{Metoda siecznych}
\begin{itemize}
    \item Wykorzystuje informacje o funkcji.
    \item Tworzy model funkcji tak żeby rozwiązać prostszy problem.
    \item Jeśli mamy funkcje to tworzymy jej model za pomocą interpolacji liniowej, czyli mając dwa punkty wyznaczamy prostą, znajdujemy jej miejsce zerowe i dostajemy jakiś punkt.
    \item Po otrzymaniu nowego punktu znowu przeprowadzamy interpolacje liniową i otrzymujemy kolejny punkt.
    \item Wadą metody siecznych jest to, że jeśli jest zbieżna to jest zbieżna, jeśli funkcja jest podobna do prostej to jest dobrze. Natomiast jeśli wystąpią jakieś wygięcia różnego rodzaju to zaczynają się schody, bo nasza funkcja może uciec nam od poszukiwanego punktu.
    \item Można go rozszerzyć i przybliżać wielomianem, ale nie wiemy które miejsce przybliżać.
    \item Jeśli chcemy zoptymalizować metodę to powinniśmy przeprowadzać interpolacje za pomocą wielomianu drugiego lub trzeciego stopnia.
    \item Uogólnia się na funkcje wielu zmiennych.
\end{itemize}

\subsection{Regula falsi}
\begin{itemize}
    \item Bierzemy sieczną, która gwarantuje nam przeciwne znaki na krańcach przedziału.
    \item Jest to lepsze, ale może być sytuacja, że jeden z krańców przedziału nie będzie nam się zawężał, bo jeżeli funkcja nie będzie miała mieć odpowiedniego kształtu to będziemy zbliżać się tylko z jednej strony i będzie problem żeby trafić w miejsce zerowe.
    \item Rysujemy sieczne, ale jeśli dwa punkty nie będą miały miejsca zerowego między sobą, to zachowujemy poprzedni punkt, w którym tak było.
    \item Ostatecznie może dojść do sytuacji, że z jednej strony dojdziemy praktycznie do miejsca zerowego, a wtedy z drugiej strony zaczniemy się zwężać.
    \item Nie uogólnia się na funkcje wielu zmiennych.
\end{itemize}

\subsection{Metoda Newtona}
\begin{itemize}
    \item Metoda stycznych
    \item Tworzymy model liniowy funkcji, ale nie interpolacją a rozwinięciem w szereg Taylora do pierwszego wyrazu
    \item Takie przybliżenie gwarantuje nam, że nasza linia prosta w pewnym otoczeniu będzie podobna do funkcji nieliniowej.
    \item Lokalnie takie przybliżenie jest dobre, mamy gwarancje podobieństwa, nie to co w metodzie siecznych.
    \item Jest w praktyce najszybszą metodą obliczania miejsca zerowego, ma zbieżność kwadratową, jest to w zasadzie najszybciej jak się da.
    \item Jeżeli metoda Newtona jak jest zbieżna to jest zbieżna, a jak nie to ucieka jak się da.
    \item Problem polega na tym, że jak pochodna zmienia znak to idzie w drugą stronę, pojawienie się pochodnej która się zbliża się do zera to stwarza problem, ponieważ przez nią dzielimy.
\end{itemize}

\section{Wartości własne i ich związek z pierwiastkami, macierz Frobeniusa}

\subsection{Wartości własne i ich związek z pierwiastkami}
\begin{itemize}
    \item Niech A będzie zespoloną macierzą kwadratową. 
    \item Niezerowy wektor zespolony $w$ jest wektorem własnym $A$, zaś $\lambda \in \mathbb{C}$ jest odpowiadającą mu wartością własną.
    \item Jest to taki wektor, na którego zadziałanie macierzą powoduje tylko jego skalowanie:
\end{itemize}
\begin{equation*}
    Aw= \lambda w
\end{equation*}
Wielomian charakterystyczny macierzy $A$ o wymiarach $m \times m$ to wielomian stopnia m o postaci:
\begin{equation*}
    p(z) = det(\lambda I-A)
\end{equation*}
Wartości własne są pierwiastkami wielomianu charakterystycznego macierzy $A$.

\subsection{Macierz Frobeniusa}
\begin{equation*}
    \begin{bmatrix}
        & 1 & & \\
        & & \ddots & \\
        & & & 1 \\
        -a_0 & -a_1 & \cdots & -a_{n-1}
    \end{bmatrix}
\end{equation*}

Macierz stowarzyszona, której wielomian charakterystyczny jest równy:

\begin{equation*}
    p(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_1\lambda+a_0
\end{equation*}

Dla każdego wielomianu jesteśmy w stanie skonstruować sobie macierz, która ma wartości własne takie jak pierwiastki wielomianu.

\begin{itemize}
    \item Wartości własne macierzy trójkątnej leżą na jej przekątnej.
    \item Ślad macierzy jest równy sumie wartości własnych.
    \item Wyznacznik macierzy jest równy iloczynowi wartości własnych.
    \item Dla każdej macierzy nieosobliwej $P$ wartości własne macierzy $A$ i $PAP^{-1}$ są sobie równe.
\end{itemize}

\section{Dekompozycje macierzy i postać Schura}

\subsection{Dekompozycje macierzy}
\begin{itemize}
    \item Każdą macierz, która ma różne wartości własne można przedstawić w postaci:
    \begin{equation*}
        A = X \Lambda X^{-1}
    \end{equation*}
    \item $X$ to macierz wektorów własnych a $\Lambda$ to macierz diagonalna wartości własnych.
    \item Na ogół wartości własne w numerycznej analizie są pojedyncze, zatem macierz wartości własnych jest diagonalna.
    \item Nie jest praktyczne robienie pełnej dekompozycji Jordana, bo wektory własne są dziwne i metoda ta jest podatna na błędy numeryczne.
    \item Każda macierz normalna (unitarna) $(A^*A = AA^*)$ może zostać przedstawiona w postaci, gdzie macierz wektorów własnych $Q$ jest macierzą unitarną, czyli wektory są ortogonalne i są długości 1:
    \begin{equation*}
        A = Q \Lambda Q^*
    \end{equation*}
    \item Jeżeli wektory własne nie będą z natury ortogonalne, to jest problem i trzeba to robić inaczej, a ortogonalne są tylko dla macierzy normalnej.
\end{itemize}

\subsection{Postać Schura}
\begin{itemize}
    \item Każdą macierz kwadratową można przedstawić w formie rozkładu Schura, gdzie $Q$ jest macierzą unitarną, zaś $T$ jest macierzą trójkątną górną.
    \begin{equation*}
        A = QTQ^*
    \end{equation*}
    \item Rozkład Schura jest podstawą wszystkich algorytmów wyliczania wartości własnych.
    \item Zaczynamy od wielomianu, tworzymy macierz, rozkładamy na postać Shura i odczytujemy wartości własne z przekątnej macierzy trójkątnej.
\end{itemize}

Zasada konstrukcji rozkładu Schura:
\begin{itemize}
    \item Konstruujemy ciąg przekształceń za pomocą macierzy unitarnych (z obu stron), tak aby ich iloczyn zmierzał do macierzy trójkątnej:
    \begin{equation*}
        \underbrace{Q_j^* \dots Q_2^*Q_1^*}_{Q^*} A \underbrace{Q_1Q_2 \dots Q_j}_{Q}
    \end{equation*}
    \item Problem ten jest bardziej skomplikowany niż eliminacja Gaussa, bo nie istnieje algebraiczny sposób przejścia na postać Shura.
    \item Wynika to z twierdzenia Abela, zatem musimy skontruować sposób iteracyjny.
    \item Stworzenie przejścia nie będzie proste, ponieważ po wyczyszczeniu pierwszej kolumny przez pomnożenie z lewej strony, to transpozycja tej macierzy i pomnożenie z prawej strony zmieni nam pierwszą kolumnę.
\end{itemize}

\begin{equation*}
    \underset{A}{
    \begin{bmatrix}
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
    \end{bmatrix}}
    \xrightarrow{Q_1^* \cdot}
    \underset{Q_1^* A}{
    \begin{bmatrix}
        \times & \times & \times & \times & \times \\
        0 & \times & \times & \times & \times \\
        0 & \times & \times & \times & \times \\
        0 & \times & \times & \times & \times \\
        0 & \times & \times & \times & \times \\
    \end{bmatrix}}
    \xrightarrow{\cdot Q_1}
    \underset{Q_1^* A Q_1}{
    \begin{bmatrix}
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
        \times & \times & \times & \times & \times \\
    \end{bmatrix}}
\end{equation*}

Praktyczne wyliczanie wartości własnych wykorzystuje dwuetapowe podejście:
\begin{itemize}
    \item Doprowadzamy macierz do postaci możliwie prostej
    \begin{itemize}
        \item Jeśli macierz nie jest symetryczna musimy doprowadzić ją do postaci Hessenberga - bliskiej trójkątnej.
        \begin{itemize}
            \item Następnie iteracyjnie dojść do macierzy trójkątnej.
        \end{itemize}
        \item Jeśli jest symetryczna to jest prościej, bo macierz Hessenberga staje się macierzą trójprzekątniową symetryczną.
        \begin{itemize}
            \item Po wyczyszczeniu doprowadzamy ją do macierzy diagonalnej.
        \end{itemize}
    \end{itemize}
    \item Praktycznie przekształcamy macierz do postaci Hessenberga, aby po pomnożeniu przez transpozycje macierz wynikowa nam się nie zmieniała.
    \item Istnieje taka transformacja za pomocą macierzy unitarnych, która przekształci nam macierz do postaci Hessenberga lub postaci trójprzekątniowej. Wymaga to od nas przeprowadzenia $m-2$ kroków. Gdzie $m$ to wymiar macierzy.
    \begin{equation*}
        \underbrace{Q^*_{m-2} \cdots Q_2^* Q_1^*}_{Q^*} A \underbrace{Q_1Q_2 \cdots Q_{m-2}}_{Q} = H
    \end{equation*}
    \item Na koniec redukujemy to co zostało iteracyjnie aż dojdziemy do liczb bliskich zeru co nas satysfakcjonuje.
\end{itemize}

\section{Macierz Hessenberga i przekształcanie do niej}
\begin{itemize}
    \item Macierz prawie trójkątna.
    \item Macierz Hessenberga, to macierz, która ma wszystkie elementy poniżej pierwszej subdiagonali równe zero. Dla macierzy symetrycznych jest to macierz trójprzekątniowa.
    \item Transformację do macierzy Hessenberga można wyznaczyć dokładnie (nieiteracyjnie) poprzez $m$ przekształceń unitarnych (z lewej i prawej strony)
    \item Każda macierz $A$ jest ortogonalnie podobna do tzw. macierzy Hessenberga:
    \begin{equation*}
        A = QHQ^*
    \end{equation*}
    \item A macierz $H$ ma postać:
\end{itemize}

\begin{equation*}
    H = 
    \begin{bmatrix}
        h_{11} & h_{12} & h_{13} & \cdots & h_{1m} \\
        h_{21} & h_{22} & h_{23} & \cdots & h_{2m} \\
        & h_{32} & h_{33} & \cdots & h_{3m} \\
        & & \ddots & \ddots & \vdots \\
        & & & h_{m,m-1} & h_{m,m} \\
    \end{bmatrix}
\end{equation*}

\begin{itemize}
    \item Dla dużych macierzy pełna dekompozycja jest zbyt kosztowna, więc używamy Iteracji Arnoldiego, która polega na rozważeniu $n$ pierwszych kolumn równania $AQ = QH$
    \item Jeżeli $Q_n$ to pierwsze $n$ kolumn $Q$ to możemy rozłożyć na:
    \begin{equation*}
        A Q_n = Q_{n+1} H_n
    \end{equation*}
    \item $H_n$ to lewa górna część macierzy $H$ o wymiarach $(n+1) \times n$:
\end{itemize}
\begin{equation*}
    H_n = 
    \begin{bmatrix}
        h_{11} & h_{12} & h_{13} & \cdots & h_{1n} \\
        h_{21} & h_{22} & h_{23} & \cdots & h_{2n} \\
        & h_{32} & h_{33} & \cdots & h_{3n} \\
        & & \ddots & \ddots & \vdots \\
        & & & h_{n,n-1} & h_{n,n} \\
        & & & & h_{n+1,n} \\
    \end{bmatrix}
\end{equation*}
\begin{itemize}
    \item Transformacja do postaci Hessenberga dla dowolnej macierzy ma złożoność $O(\frac{10}{3}m^3)$, czyli wolniej trochę niż eliminacja Gaussa.
    \item Dla macierzy symetrycznych możliwa jest redukcja do $O(\frac{4}{3}m^3)$
    \item Algorytm transformacji do macierzy Hessenberga, jest stabilny wstecznie.
\end{itemize}

\section{Iteracja potęgowa, iteracja odwrotna i iloraz Rayleigha}
\subsection{Iteracja potęgowa}
\begin{itemize}
    \item Niech macierz A ma wektory własne $q_1, q_2, \dots ,q_n$ i $m$ różnych wartości własnych. Wtedy ciąg wektorów jest zbieżny do wektora własnego odpowiadającego największej co do modułu wartości własnej.
    \begin{equation*}
        \frac{v_0}{|v_0|} , \frac{Av_0}{|Av_0|}, \frac{A^2v_0}{|A^2v_0|}, \dots
    \end{equation*}
    \item Jeżeli weźmiemy jakiś wektor $v$, znormalizujemy go i pomnożymy przez $A$ i będziemy robić tak w kółko to ciąg ten będzie nam zmierzał do wektora własnego związanego z największą wartością własną.
    \item Dzieje się tak ponieważ, kiedy macierz ma różne wartości własne to wektory własne są liniowo niezależne, jeżeli są liniowo niezależne to każdy $n$ wymiarowy wektor jesteśmy w stanie przedstawić jako $n$ wektorów własnych.
    \item Wynika z tego że nasz wektor $v$ będzie równy kombinacji wektorów własnych.
    \item Iteracja potęgowa zmierza do wyliczenia wartości własnej.
    \item Znając wektor własny możemy wyliczyć wartość własną z ilorazu Rayleigha.
    \item Iteracja potęgowa zbiega tylko do wektora związanego z największą wartością własną.
    \item Iteracja potęgowa ma zbieżność liniową, z kroku na krok poprawa następuje proporcjonalnie do ilorazu największej wartości własnej i kolejnej.
    \item Występuje problem, gdy wartości własne są bliskie sobie.
\end{itemize}

\subsection{Iteracja odwrotna}
\begin{itemize}
    \item Dla każdej rzeczywistej liczby $\mu \in \mathbb{R}$, która nie jest wartością własną macierzy $A$ wektory własne macierzy $(A - \mu I)^{-1}$ są takie same jak wektory własne macierzy $A$.
    \item Odpowiadające im wartości własne to $(\lambda_j - \mu)^{-1}$ gdzie $\lambda_j$ to wartości własne macierzy $A$.
    \item Jeżeli od naszej macierzy odejmiemy sobie macierz jednostkową pomnożoną przez jakąś liczbę, to jej odwrotność ma dokładnie takie same wektory własne co ta macierz.
    \item Jeżeli $\mu$ jest bliskie wartości własnej $\lambda_J$, wtedy $(\lambda_J - \mu)^{-1}$ będzie dużo większe od $(\lambda_j - \mu)^{-1}$ dla różnych $j$ i $J$.
    \item Nowe wartości własne dla $\mu$ bliskiego wartości własnej będą bardzo szybko zbiegać do $q_J$.
    \item Dochodzimy do najbliższej wartości własnej od obranego $\mu$.
\end{itemize}

\subsection{Iloraz Rayleigha}
\begin{itemize}
    \item Jak wyliczyć wartość własną macierzy znając związany z nią wektor własny.
    \begin{equation*}
        r(x) = \frac{x^*Ax}{x^*x}
    \end{equation*}
    \item Łatwo zauważyć, że jeśli x jest wektorem własnym to $r(x) = \lambda$, gdzie $\lambda$ to odpowiadająca mu wartość własna. \item Jeżeli $\|x\| = 1$ to $r(x)=x^TAx$
    \item Iloraz Rayleigha dla znanego nam wektora własnego jest wartością własną.
    \item Błąd wyznaczania wartości własnej w taki sposób jest dość mały.
\end{itemize}

\subsection{Połączenie podejść}
Aby efektywnie obliczyć wartości własne należy połączyć trzy metody i stworzyć algorytm, który je wykorzystuje.
\begin{itemize}
    \item Iloraz Rayleigha pozwala na wyznaczenie wartości własnej znając wektor własny.
    \item Iteracja odwrotna pozwala na wyznaczenie wektora własnego znając przybliżenie wartości własnej.
    \item Jeśli będziemy używać coraz lepszego przybliżenia wartości własnej to dominacja $(\lambda_j - \mu)^{-1}$ będzie jeszcze większa, przez co szybciej dojdziemy do wartości, która nas interesuje.
    \item Aby wyliczyć wartości własne stosujemy tzw. iteracje ilorazu Rayleigha, która polega na naprzemiennym stosowaniu obydwu algorytmów.
    \item Jest to bardzo szybki sposób wyliczania wartości własnych.
    \item Jest to jeden z najszybciej zbieżnych algorytmów numerycznych. Zarówno wartości jak i wektory własne zbiegają w sposób sześcienny dla odpowiednio bliskich przybliżeń.
    \item Niestety trzeba iterować po różnych wartościach startowych, żeby otrzymać różne wartości własne.
\end{itemize}

\section{Algorytm QR wyznaczania wartości własnych (Zasada działania i rząd zbieżności)}
Bezpośrednie poszukiwanie wartości własnych iteracją ilorazu Rayleigha jest utrudnione ze względu na zależność od wektora początkowego. Stąd pomysł, żeby szukać w różnych kierunkach jednocześnie - najlepiej ortogonalnych.
\subsection{Zasada działania}
\begin{itemize}
    \item Do konstrukcji poszukiwań zastosujemy rozkład QR, dla macierzy $A$:
    \begin{equation*}
        \begin{split}
            A_0 & = A \\
            Q_k R_k & = A_{k-1} \\
            A_k & = R_k Q_k
        \end{split}
    \end{equation*}
    \item Iteracyjnie rozkładamy macierz rozkładem QR a następnie składamy ją w odwrotnej kolejności.
    \item Kończymy składanie wtedy, kiedy suma elementów w pierwszej poddiagonali będzie dostatecznie bliska zeru, wtedy otrzymaliśmy macierz trójkątną.
    \item W zredukowanej macierzy bliskiej trójkątnej otrzymujemy wartości własne na przekątnej.
    \item Algorytm ten jest zbieżny do postaci Shura, ale wolno.
\end{itemize}

\subsection{Praktyczny algorytm QR}
\begin{itemize}
    \item Rozpoczyna od macierzy w postaci Hessenberga, pozbywamy się algebraicznie wszystkiego, co tylko możliwe żeby jak najbardziej upodobnić się do macierzy trójkątnej.
    \item Wykorzystuje iterację ilorazu Rayleigha dla przyspieszenia zbieżności.
    \item W momencie pojawienia się wartości bliskich zeru pod przekątną macierz jest rozdzielana na podproblemy jest to deflacja.
\end{itemize}

\subsection{Przesunięcia}
\begin{itemize}
    \item Wykorzystujemy w każdej iteracji przybliżenie wartości własnej z ilorazu Rayleigha $\mu_k$
    \item Skupiamy się na jednej wartości własnej i ją przybliżamy w oparciu o iloraz Rayleigha.
    \item Następnie wycinamy mniejszą macierz i liczymy dalej, aż do końca.
    \item Przesuwamy i wsuwamy wartości własne, żeby dostać tą samą sytuacje.
    \item Macierze $Q$ zbiegają do macierzy wektorów własnych, zaś przekątna macierzy $R$ do ilorazów Rayileigha. Dzięki temu jako element $\mu_k$ wybieramy element z przekątnej.
    \item Wykonujemy algorytm QR z przesunięciami wraz z deflacją.
\end{itemize}

\subsection{Rząd zbieżności}
Algorytm jest zbieżny, ale wolno i zależnie od wzajemnych stosunków wartości własnych do siebie. Koszt wyznaczenia wszystkich wektorów i wartości własnych jest rzędu $O(n^3)$ z pewną stała.

\section{Aproksymacja wielomianowa, szereg Czebyszewa, interpolacyjne wyznaczanie pierwiastków funkcji}
\subsection{Aproksymacja wielomianowa}
Aproksymacja wielomianowa, to sposób taka metoda aproksymacji, w której zakładamy, że funkcja przybliżająca ma postać wielomianu.

\subsection{Szereg Czebyszewa}
Wyznaczamy współczynniki szeregu Czebyszewa za pomocą transformaty Fouriera, aż będą one bliskie zera maszynowego.

\subsection{Interpolacyjne wyznaczanie pierwiastków funkcji}
\begin{itemize}
    \item Konstruujemy macierz koleżeńską - odpowiednik macierzy Frobeniusa dla wielomianów Czebyszewa.
    \item Wtedy pierwiastki wielomianu określonego przez szereg Czebyszewa są równe wartościom własnym takiej macierzy:
\end{itemize}
\begin{equation*}
    C = 
    \begin{bmatrix}
        0 & 1 & & & & \\
        \frac{1}{2} & 0 & \frac{1}{2} & & & \\
        & \frac{1}{2} & 0 & \frac{1}{2} & & \\
        & & \ddots & \ddots & \ddots & \\
        & & & & \ddots & \frac{1}{2} \\
        & & & & \frac{1}{2} & 0 \\
    \end{bmatrix}
    - \frac{1}{2c_n}
    \begin{bmatrix}
        & & & & & \\
        & & & & & \\
        & & & & & \\
        & & & & & \\
        & & & & & \\
        c_0 & c_1 & c_2 & \cdots & c_{n-2} & c_{n-1} \\
    \end{bmatrix}
\end{equation*}
\begin{itemize}
    \item Jest to macierz skonstruowana za pomocą współczynników szeregu Czebyszewa
    \item Kiedy wyliczymy wartości własne takiej macierzy otrzymamy pierwiastki wielomianu Czebyszewa
    \item Dostaniemy tyle pierwiastków jaki był stopień interpolacji
    \item Jeżeli weźmiemy tylko pierwiastki zawarte w przedziale interpolacji to dostaniemy miejsca zerowe oryginalnej funkcji z dokładnością do jakości interpolacji.
\end{itemize}

\section{Metoda Newtona rozwiązywania układów równań i warianty, metoda Broydena}

\subsection{Metoda Newtona rozwiązywania układów równań i warianty}
\begin{itemize}
    \item Polega na rozwinięciu residuum w szereg Taylora
    \item Chcemy przeformułować problem nieliniowy na liniowy.
    \item Kolejne kroki wymagają wprowadzenia rachunku tensorowego.
    \item Idea polega na znalezieniu takiego $p$, żeby $r(x_k+p) = 0$, czyli $p$ dla którego residuum się wyzeruje.
    \begin{equation*}
        J_kp=-r(x_k)
    \end{equation*}
    \item Wtedy naszym rozwiązaniem będzie $x_k + p$
    \item $J_k$ to macierz Jakobiego pochodnych cząstkowych
    \item Jeżeli macierz będzie osobliwa to nie ma rozwiązania.
    \item Jeżeli jest nieosobliwa, ale bliska osobliwej to $p$ może być duże, to tracimy zbieżność metody Newtona.
    \item Jeżeli jesteśmy blisko rozwiązania a pochodna spełnia warunek Lipshitza to mamy zbieżność kwadratową.
    \item Chcemy się zbliżać do rozwiązania i zachować zbieżność.
\end{itemize}

\subsection{Niedokładne metody Newtona}
\begin{itemize}
    \item Szukajmy takiego $p$, żeby residuum było mniejsze niż w kroku poprzednim.
    \item Nie chcemy sytuacji, że coś nam odleci, czyli stracimy zbieżność.
    \item Z kroku na krok zmieniamy stopień zbliżania się do szukanego rozwiązania.
    \item Na początek mniej, a jak jesteśmy blisko to możemy zbliżać się szybciej.
    \item Następna aproksymacja residuum musi być mniejsza od poprzedniej i to jest pomysł, który chcemy wykonywać.
    \item Jest to analogiczne postępowanie do Regula falsi.
\end{itemize}

\subsection{Metoda Broydena}
\begin{itemize}
    \item Jest to uogólnienie metody siecznych
    \item Konstruujemy taką aproksymację macierzy Jakobiego, która będzie spełniać tzw. warunek, siecznej, czyli zmiana residuum jest liniową proporcją macierzy do zmiany argumentów.
    \item Warunek siecznej to równość dwóch wektorów, czyli musi się nam zgadzać n elementów.
    \item Można stworzyć nieskończenie wiele aproksymacji macierzy, które będą działały tak jak chcemy.
    \item Najpopularniejsza aproksymacja macierzy Jakobiego, jest najmniejszą możliwą zmianą w aproksymacji przy spełnieniu warunku siecznej.
    \item Szukamy takiej funkcji, która reprezentuje nam zmianę residuum jako macierz razy zmiana argumentów, ale jest bardzo podobna do poprzedniej macierzy.
\end{itemize}

\section{Całkowanie w czasie rzeczywistym, Metody Eulera, trapezów}

\subsection{Całkowanie w czasie rzeczywistym}
\begin{itemize}
    \item Domena automatyki np. w regulacji PID.
    \item Najważniejsza rzeczą jest, kompensacja błędów o niskich częstotliwościach.
    \item Otrzymujemy konkretne wartości funkcji w określonych odstępach czasu.
    \item Nie możemy wybierać punktów, dostajemy coś i nie możemy tego zmienić
    \item Wartości funkcji są obarczone błędem pomiarowych
\end{itemize}

\subsection{Metody Eulera}
\begin{itemize}
    \item Najprostszy sposób obliczania całki
    \item Przybliżamy całkę przy pomocy poprzedniej wartości pomnożonej przez długość kroku.
    \item Wyciągamy wartości do przodu
    \item Aproksymujemy wartość całki poprzez wartość poprzedniego kroku i zakładamy, że na najbliższym przedziale kroku funkcja jest funkcją stałą, mnożymy wartość i odstęp między próbkami
    \item Dostajemy w ten sposób przybliżenie całki, które z kroku na krok będzie dawać nam kolejną aproksymacje całki wykorzystując informacje poprzednią i dodaniu nowego kawałka.
    \item Używa się jej do liczenia transmitancji układów dyskretnych.
    \item Jest ona bardzo prosta do wyliczenia.
    \item Można ją przenieść na układy nieliniowe.
    \item Nie mamy gwarancji stabilności dla układu dyskretnego, ciągły jest stabilny a dyskretny może nie być, trzeba kontrolować odstęp między próbkami.
    \item Jest mało dokładna - równa z aproksymacją szeregiem Taylora za pomocą pierwszego elementu.
\end{itemize}

\subsection{Metody trapezów}
\begin{itemize}
    \item Wykorzystujemy wartości zarówno poprzednią jak i obecną, możemy przyjąć że funkcja pomiędzy próbkami była liniowa.
    \item Chcemy rozwiązać problem za pomocą prostszego problemu.
    \item Wykorzystujemy to co doliczyliśmy do tego momentu i dodajemy kolejną wartość.
    \item Do poprzedniej aproksymacji całki dodajemy wzór na pole trapezu, wartości na brzegach pomnożone przez $\frac{1}{2}$ odległości pomiędzy próbkami.
\end{itemize}

\section{Kwadratury interpolacyjne, Rząd wielomianowy kwadratury}

\subsection{Kwadratury interpolacyjne}
\begin{itemize}
    \item Kwadratura to numeryczne wyliczenie całki (quadrature)
    \item Chcemy przedstawić wartość całki na przedziale jako ważoną sumę wartości funkcji w punktach.
    \item Wyliczamy wartości w punktach, przy czym każdy punkt ma swoją wagę.
\end{itemize}

\begin{equation*}
    \int_{-1}^{1} f(t) dt = \sum_{i=0}^{n} w_i f(t_i)
\end{equation*}

\begin{itemize}
    \item Wykorzystują tak jak interpolacja - wartości funkcji w punktach.
    \item Naturalnym sposobem na wyliczenie wag i dostanie wyniku całki jest interpolacja funkcji wielomianem i wyliczenie całki z tego wielomianu.
\end{itemize}

\subsection{Rząd wielomianowy kwadratury}
\begin{itemize}
    \item Kwadratura ma rząd wielomianowy $m$, jeżeli dla wszystkich wielomianów stopnia $m$ kwadratura jest równa dokładnie wartości całki.
    \item Ważne jest to dlatego, że każdą przyzwoitą funkcję możemy rozwinąć w szereg Taylora, czyli przybliżyć ją wielomianem. Teoretycznie, jeśli wyliczenie będzie dokładne to nasza całka będzie wyliczona bardzo dobrze.
\end{itemize}

\section{Kwadratura Clenshawa-Curtisa Wyznaczanie wag i węzłów.}
\begin{itemize}
    \item Wykorzystuje wszystko związane z wielomianami Czebyszewa
    \item Oparta na węzłach Czebyszewa
    \item Złożoność wyliczenia kwadratury $O(n log(n))$
    \item Dokładność będzie gorsza niż Legendre'a, zbieżność nie ma tak regularnego zachowania.
    \item Błąd kwadratury jest podobny do błędu interpolacji, dla funkcji analitycznych jest okej.
    \item Bardzo wydajna dla funkcji o dużej regularności.
    \item Aby wyliczyć kwadraturę CC musimy:
    \begin{itemize}
        \item Wyliczyć wielomian interpolacyjny
        \item Za pomocą FFT zamieniamy go na szereg Czebyszewa
        \item Całki z poszczególnych wielomianów Czebyszewa znamy, więc obliczenie całki to suma iloczynów współczynników Czebyszewa i odpowiadających im wartości całek.
    \end{itemize}
\end{itemize}

\subsection{Wyznaczanie wag i węzłów}
\begin{itemize}
    \item Wagi to współczynniki szeregu Czebyszewa
    \item Węzły takie jak w interpolacji Czebyszewa
\end{itemize}

\section{Kwadratura Gaussa Legendre'a. Wyznaczanie wag i węzłów kwadratury.}
\begin{itemize}
    \item Wielomiany Legendre'a są ortogonalne do siebie dla iloczynu skalarnego z funkcją wagową równą $1$. Iloczyn ten jest równy całce od $-1$ do $1$ z iloczynu funkcji.
    \item Spełniają one relacje rekurencyjną, wielomian stopnia $k+1$ da się wyrazić za pomocą wielomianów $k$ i $k-1$.
    \item Dla każdego $n \geq 0$ kwadratura interpolacyjna na $n + 1$ węzłach ma rząd wielomianowy $n$. Kwadratura Gaussa-Legendre’a ma rząd wielomianowy $2n + 1$. Jest ona dwa razy dokładniejsza niż interpolacja pod względem rzędu.
    \item Dokonując obliczeń za pomocą kwadratury problem się redukuje do wyliczania kwadratury niższego stopnia.
    \item Kwadratura Gaussa-Legendre’a jest najdokładniejsza dla wielomianów
    \item Wszystkie kwadratury Gaussa mają w zasadzie niepowtarzalne węzły, węzły kwadratury rzędu $n$ i $n+1$ rzadko są takie same, więc podniesienie rzędu kwadratury wymaga wyliczenia wartości w nowych węzłach.
\end{itemize}

\subsection{Wyznaczanie wag i węzłów kwadratury}
\begin{itemize}
    \item Węzły Legendre'a to miejsca zerowe wielomianów Legendre'a.
    \item Są one optymalnymi węzłami interpolacji, wprowadzają one najmniejszy błąd interpolacji.
    \item Wyliczanie ich jest utrudnione.
    \item Węzły i wagi można wyliczyć jedynie numerycznie, co ma złożoność $O(n^3)$
\end{itemize}

\subsection{Algorytm Goluba-Welscha}
\begin{itemize}
    \item Węzły i wagi, można wyliczyć rozwiązując pewien problem własny, działa to dla wszystkich rodzajów wielomianów ortogonalnych które da się przedstawić za pomocą rekurencji.
    \item Robi się to za pomocą równania macierzowego, gdzie uwzględniamy rekurencje.
    \item Po przekształceniach dochodzimy do tego, że wektor wielomianów będzie wektorem własnym ww. macierzy.
    \item Waga pomnożona przez iloczyn wektorów własnych w danym punkcie jest równa $1$, z czego możemy ją wyliczyć.
    \item Ten sposób wyliczeń posiada dużą złożoność obliczeniową
\end{itemize}

\subsection{Algorytm Hale’a - Towsenda}
\begin{itemize}
    \item Najszybszy sposób obliczenia wielomianów Legendre'a
    \item Ma on dla dużych $n$ złożoność liniową.
    \item Dla wielomianów Legendre'a istnieją wzory asymptotyczne dla dużych $n$, które podają nam do jakich wartości zmierzają nam wartości pierwiastków.
    \item Zaletą asymptotycznych wzorów na pierwiastki jest to, że możemy znacząco zredukować obliczenia, wzory te są chore jakieś.
    \item Wystarczy się skupić na pierwiastkach w przedziale $[0, 1]$
    \item Musimy skupić się na dwóch grupach, oddalonych od $1$ i bliskich $1$.
    \item Kiedy możemy uzyskać przybliżone wartości pierwiastków, możemy w tych przybliżonych wartościach pierwiastków wyliczyć wielomian oraz jego pochodną.
    \item Wyliczenie wartości wielomianu wysokiego stopnia może być kosztowne obliczeniowo.
    \item W takich sytuacjach wprowadzamy wzory asymptotyczne na wartość wielomianu, które mają mniejszą złożoność obliczeniową.
    \item Jeżeli mamy jakieś przybliżenie pierwiastków to możemy obliczyć wartość wielomianu.
    \item Aby wyliczyć pochodne używamy rekurencji, bo pochodna wielomianu Legendre'a może być wyrażona za pomocą sumy dwóch wielomianów Legendre'a stopnia $n$ i $n-1$.
    \item Dla dużego $n$ możemy przybliżyć wartość pochodnej.
    \item Startujemy z przybliżenia rozwiązania, stosujemy metodę Newtona i sprawdzamy, czy dostaliśmy pierwiastek wielomianu, jeśli tak to obliczamy wagę, a jeżeli nie to wykonujemy kolejny krok Newtona.
\end{itemize}

\subsection{Wagi kwadratury}
Jak już mamy wszystko możemy wyliczyć wagi ze wzorów, który wykorzystuje węzły i wartości pochodnej.

\section{Kwadratury adaptacyjne (zasada działania)}
\begin{itemize}
    \item Idea kwadratury adaptacyjnej polega na tym, aby liczyć kwadratury w podprzedziałach.
    \item Szukamy dwóch kwadratur różnych rzędów, które mają węzły na brzegach przedziału (i jak najwięcej wspólnych) - aby koszt obliczeń był mały.
    \item Jeśli wartości tych dwóch kwadratur są zbliżone to uznajemy, że całka została policzona dokładnie i kończymy liczenie.
    \item Jeżeli różnica między kwadraturami jest duża, to stosujemy każdą z kwadratur na podprzedziałach pomiędzy już wyliczonymi węzłami.
    \item W każdym z tych przedziałów stosujemy wybrane kwadratury, jeśli policzymy to okej, a jak nie to powtarzamy proces.
\end{itemize}

\section{Istnienie i jednoznaczność rozwiązania równania różniczkowego, warunek Lipschitza}
\begin{itemize}
    \item Równanie różniczkowe ma rozwiązanie i jest ono jednoznaczne wtedy i tylko wtedy, gdy funkcja $f$ spełnia warunek Lipshitza.
    \item Warunek musi być spełniony ze względu na zmienną zależną.
    \begin{equation*}
        |f(x,z)-f(x,y)| \leq L|z-y|
    \end{equation*}
    \item Warunek Lipshitza jest mocniejszą ciągłością, ale słabszy niż różniczkowalność.
    \item Bazuje na twierdzeniu o punkcie stałym.
    \item Brak spełnienia warunku Lipschitza sprawia, że rozwiązania mogą prowadzić nas w różne, dowolne strony.
\end{itemize}

\section{Metoda eulera w przód, dokładność, błąd, łamana Eulera}
\subsection{Metoda Eulera w przód}
\begin{itemize}
    \item Każdą funkcje jesteśmy w stanie rozwinąć w szereg Talora
    \item Szereg wokół punktu $x_0$ będzie składał się z rozwinięcia poszczególnych elementów.
    \item Jeśli prawa strona równania różniczkowego jest dostatecznie razy różniczkowalna, to szereg Taylora jest dokładnym rozwiązaniem równania różniczkowego. W zakresie zbieżności szeregu, który dla funkcji nieskończenie razy różniczkowalnych jest odpowiednio duży.
    \item Rozwijamy naszą funkcje wokół punktu $x_0$, w takiej sytuacji po rozwinięciu otrzymamy:
    \begin{equation*}
        y(x-x_0)=y_0 +(x-x_0)y'(x_0)+ \cdots = y_0 (x-x_0)f(x_0,y_0) + \cdots
    \end{equation*}
    \item Rozwinięcie działa dla małych $x$, im większe tym gorzej.
\end{itemize}

\subsection{Metoda prostokątów Eulera}
\begin{itemize}
    \item Ma na celu znalezienie rozwiązania równania różniczkowego startując z punktu $(x_0, y_0)$, w punkcie $y(X)$ wykorzystując do tego ciąg punktów. Dzielimy przedział rozwiązania na podprzedziały.
    \item Na każdym z podprzedziałów będziemy stosować pierwszy wyraz szeregu Taylora zakładając, że między naszymi punktami funkcja wynikowa jest dobrze reprezentowana przez prostą (styczną) - chcemy dostatecznie dobrze dokleić się do funkcji
    \item Zastępujemy sobie kolejny wyraz poprzednimi:
    \begin{equation*}
        \begin{split}
            y_1 - y_0 & = (x_1-x_0)f(x_0,y_0) \\
            y_2 - y_1 & = (x_2-x_1)f(x_1,y_1) \\
            & \cdots
        \end{split}
    \end{equation*}
    \item Zastępujemy tak aż do momentu aż $x_n = X$
    \item Zakładamy, że kroki pomiędzy przejściami mogą być dowolnie długie, jest to odległość pomiędzy kolejnymi punktami.
    \item Funkcja $y(x)$ w odległości między punktem $x_0$ a $x_1$ jest dość dobrze reprezentowana przez styczną.
\end{itemize}

\subsection{Łamana Eulera}
\begin{itemize}
    \item Jest to połączenie prostymi poszczególnych punktów.
    \item Chcemy dostać wartość $y$, pomiędzy punktami które przyjęliśmy np. pomiędzy $x_1$ i $x_2$.
    \item Przyjmujemy, że pomiędzy tymi punktami poprowadzono linię prostą.
\end{itemize}

\subsection{Zbieżność}
\begin{itemize}
    \item Jeżeli funkcja $f$ jest ciągła i ograniczona przez $A$ i spełnia warunek Lipshitza na zbiorze punktów, którego argumenty ogranicza pewien prostokąt o bokach w $x_0$ i $X$ a wartości pewien walec wyśrodkowany w punkcie $y_0$ i promieniu $b$. Dla każdego punktu mamy $x$ i $y$ ograniczone z góry i z dołu.
    \item Dla $X - x_0 \leq \frac{b}{A}$ które będzie nie większe niż ograniczenie maksymalne wzrostu zachodzi nam:
    \begin{itemize}
        \item Zmniejszając krok, łamana Eulera zmierza jednostajnie do pewnej ciągłej funkcji $\varphi(x)$ 
        \item $\varphi(x)$ jest ciągle różniczkowalnym rozwiązaniem równania na przedziale $[x_0, X]$, czyli wiemy, że krok zmierza do pewnego rozwiązania równania różniczkowego
    \item Wiemy także że nie istnieją inne rozwiązania równania na tym przedziale, bo warunek Lipshitza gwarantuje nam jednoznaczność rozwiązania.
    \end{itemize}
    \item Nasza metoda pozwala nam do zbieganie do rozwiązania. Jeżeli będziemy skracać kroki to będziemy się zbliżać do jedynego rozwiązania, pod warunkiem, że nasza funkcja jest ciągła i ograniczona oraz spełnia warunek Lipshitza.
    \item Gwarantuje nam to, że nasza aproksymacja jest asymptotycznie poprawna, popełniamy błąd odcięcia, ale gwarantujemy sobie, że do tego rozwiązania jesteśmy w stanie dojść z określoną przez nas dokładnością.
\end{itemize}

\subsection{Dokładność}
\begin{itemize}
    \item Aby osiągnąć dokładność 3 miejsc po przecinku potrzeba tysiąc kroków.
    \item Aby osiągnąć dokładność 6 miejsc po przecinku potrzeba milion kroków.
    \item Jeżeli chcemy osiągnąć sensowną dokładność musimy wykonać bardzo dużo obliczeń.
\end{itemize}

\subsection{Błąd}
\begin{itemize}
    \item Możemy go oszacować pod warunkiem, że $f$ oraz pochodne cząstkowe ze względu na $x$ i $y$ są ograniczone.
    \item Wtedy dla dostatecznie małych kroków, odległość między rozwiązaniem a łamaną Eulera jest szacowane z góry przez stałą zależną od ograniczeń $f$ i pochodnych cząstkowych oraz funkcję wykładniczą pomnożoną przez długość kroku.
    \item Całkowity (globalny) błąd metody Eulera jest proporcjonalny do maksymalnej długości kroku. Im mniejszy krok tym lepiej.
\end{itemize}

\section{Idea metod Rungego-Kutty, przykład metody punktu środkowego}

\subsection{Metoda punktu środkowego}
\begin{itemize}
    \item Jej idea polega na całkowaniu
\end{itemize}
\begin{equation*}
    y'=f(x), y(x_0)=y_0
\end{equation*}
\begin{equation*}
    y(X) = y_0 + \int_{x_0}^{X} f(x) dx
\end{equation*}
\begin{itemize}
    \item Wartość rozwiązania równania różniczkowego w kolejnych krokach będzie zależeć od wartości w połowie przedziału.
    \item Wartość w punkcie ostatnim będzie wartością poprzednią dodać krok razy wartość wzięta z połowy kroku.
    \item Ten sam mechanizm co w metodzie Eulera, ale wartość która obliczamy nie jest wartością w konkretnym kroku, jest ona przesunięta o $\frac{1}{2}$ kroku.
\end{itemize}
\begin{equation*}
    \begin{split}
        y(x_0 + h_0) \approx y_1 & = y_0 + h_0 f(x_0 + \frac{h_0}{2} ) \\
        y(x_1 + h_1) \approx y_2 & = y_1 + h_1 f(x_1 + \frac{h_1}{2} ) \\
        & \cdots \\
        y(X) \approx Y & = y_{n-1} + h_{n-1} f(x_{n-1} + \frac{h_{n-1}}{2} ) \\
    \end{split}
\end{equation*}

W idealnym przypadku mielibyśmy:
\begin{equation*}
    y(x_0 + h) \approx y_0 + h f(x_0 + \frac{h_0}{2}, y(x_0 + \frac{h}{2}))
\end{equation*}

Aby otrzymać wartość $y(x_0 + \frac{h}{2})$ należy skorzystać z metody Eulera używając małego kroku:
\begin{equation*}
    \begin{split}
        k_1 & = f(x_0,y_0) \\
        k_2 & = f(x_0 + \frac{h}{2}, y_0 +\frac{h}{2} k_1) \\
        y_1 & = y_0 + hk_2
    \end{split}
\end{equation*}

Metoda Eulera generuje pewien błąd, natomiast jest on dość mocno minimalizowany. \\ 

Analizując błąd z wykorzystaniem szeregu Taylora otrzymujemy:
\begin{equation*}
    \| y(x_0 + h) - y_1 \| \leq K h^3
\end{equation*}

Różnica między otrzymanymi wartościami jest ograniczona przez:
\begin{itemize}
    \item Pewną stała $K$ (zależną od odpowiednich pochodnych)
    \item Wartość kroku $h$
\end{itemize}

\subsection{Metoda Rungego-Kutty}
\begin{itemize}
    \item Możemy uogólnić sytuację jako całą klasę metod.
    \item Metoda ma s etapów, s-etapowa otwarta metoda RK ma postać:
\end{itemize}

\begin{equation*}
    \begin{split}
        k_1 & = f(x_0, y_0) \\
        k_2 & = f(x_0 + c_2h, y_0 + ha_{21}k_1) \\
        k_3 & = f(x_0 + c_3h, y_0 + h(a_{31}k_1 + a_{32}k_2)) \\
        & \cdots \\
        k_s & = f(x_0 + c_sh, y_0 + h(a_{s1}k_1 + \cdots + a_{s,s-1}k_{s-1})) \\
        y_1 & = y_0 + h(b_1k_1 + \cdots + b_sk_s)
    \end{split}
\end{equation*}

\begin{itemize}
    \item Wyliczamy wartości pochodnych w kolejnych punktach a nasze rozwiązanie końcowe w pewien sposób od nich zależy, jest pomnożone przez odpowiednie współczynniki.
    \item Nasze końcowe rozwiązanie będzie pewną kombinacją etapów z wagami pomnożonymi razy krok $h$.
    \item Metody Rungego-Kutty ma rząd $p$, jeśli dla odpowiednio gładkich problemów - muszą istnieć wszystkie odpowiednie pochodne zapewniające pewien poziom regularności rozwiązania.
    \item Im wyższy rząd metody, tym dokładniejszy wynik.
    \item Rząd metody to maksymalna potęga do której możemy podnieść krok by nadal był on większy od błędu uzyskanego przy obliczeniach.
\end{itemize}

\begin{equation*}
    \|y(x_0 + h) - y_1 \| \leq K h^{p+1}
\end{equation*}

\end{document}
